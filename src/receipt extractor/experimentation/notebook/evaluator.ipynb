{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = YOLO(\"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/Server/model/versions/0.2/receipt_extractor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/receipt extraction model.v2i.yolov8/test/images/0676aa86-IMG_20231221_163400_jpg.rf.3ef3b7a564323dc7d96c82e257c4c400.jpg\"\n",
    "annotation_path = \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/receipt extraction model.v2i.yolov8/test/labels/0676aa86-IMG_20231221_163400_jpg.rf.3ef3b7a564323dc7d96c82e257c4c400.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/receipt extraction model.v2i.yolov8/test/images/0676aa86-IMG_20231221_163400_jpg.rf.3ef3b7a564323dc7d96c82e257c4c400.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 538.1ms\n",
      "Speed: 0.9ms preprocess, 538.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "{'shop_information': {'conf': tensor([0.8474]), 'class_label': 'shop_information', 'class_id': 2, 'coordinates': {'x1': 194, 'y1': 129, 'x2': 462, 'y2': 209}}, 'date': {'conf': tensor([0.7863]), 'class_label': 'date', 'class_id': 0, 'coordinates': {'x1': 150, 'y1': 212, 'x2': 314, 'y2': 234}}, 'total': {'conf': tensor([0.7854]), 'class_label': 'total', 'class_id': 3, 'coordinates': {'x1': 140, 'y1': 453, 'x2': 472, 'y2': 477}}, 'item_purchase': {'conf': tensor([0.6117]), 'class_label': 'item_purchase', 'class_id': 1, 'coordinates': {'x1': 132, 'y1': 245, 'x2': 491, 'y2': 411}}}\n"
     ]
    }
   ],
   "source": [
    "def get_highest_prediction(model, image_path)-> dict:\n",
    "    # Get the predictions\n",
    "    predictions = model(image_path)\n",
    "\n",
    "    # Get the highest prediction\n",
    "    highest_conf_detections = {}\n",
    "    for result in predictions:\n",
    "        # print(result)\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls)\n",
    "            class_label = result.names[int(box.cls)]\n",
    "            conf = box.conf\n",
    "\n",
    "            # Check if the class label is already in the dictionary\n",
    "            if class_label in highest_conf_detections:\n",
    "                # Check if the current confidence is higher than the one stored\n",
    "                if conf > highest_conf_detections[class_label][\"conf\"]:\n",
    "                    highest_conf_detections[class_label] = {\n",
    "                    \"conf\": conf,\n",
    "                    \"class_label\": class_label,\n",
    "                    \"class_id\": class_id,\n",
    "                    \"coordinates\": {\n",
    "                        \"x1\": int(box.xyxy[0][0]),\n",
    "                        \"y1\": int(box.xyxy[0][1]),\n",
    "                        \"x2\": int(box.xyxy[0][2]),\n",
    "                        \"y2\": int(box.xyxy[0][3]),\n",
    "                    }\n",
    "                }\n",
    "            else:\n",
    "                highest_conf_detections[class_label] = {\n",
    "                    \"conf\": conf,\n",
    "                    \"class_label\": class_label,\n",
    "                    \"class_id\": class_id,\n",
    "                    \"coordinates\": {\n",
    "                        \"x1\": int(box.xyxy[0][0]),\n",
    "                        \"y1\": int(box.xyxy[0][1]),\n",
    "                        \"x2\": int(box.xyxy[0][2]),\n",
    "                        \"y2\": int(box.xyxy[0][3]),\n",
    "                    }\n",
    "                }\n",
    "\n",
    "\n",
    "    return highest_conf_detections\n",
    "\n",
    "extracted_prediction = get_highest_prediction(model, image_path)\n",
    "print(extracted_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shop_information': {'x1': 129, 'y1': 125, 'x2': 414, 'y2': 179},\n",
       " 'item_purchase': {'x1': 72, 'y1': 179, 'x2': 449, 'y2': 367},\n",
       " 'total': {'x1': 96, 'y1': 366, 'x2': 420, 'y2': 382},\n",
       " 'date': {'x1': 59, 'y1': 545, 'x2': 244, 'y2': 565}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def convert_yolo_coordinates(file_path, img_width, img_height)->dict:\n",
    "\n",
    "    class_mapping = {\n",
    "        0: \"date\",\n",
    "        1: \"item_purchase\",\n",
    "        2: \"shop_information\",\n",
    "        3: \"total\",\n",
    "    }\n",
    "        \n",
    "    labels_coordinates = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) != 5:\n",
    "            continue  # Skip invalid lines\n",
    "\n",
    "        class_id, x_center, y_center, width, height = map(float, parts)\n",
    "\n",
    "        # Calculate the coordinates of the bounding box (x1, y1, x2, y2)\n",
    "        x1 = int((x_center - width / 2) * img_width)\n",
    "        y1 = int((y_center - height / 2) * img_height)\n",
    "        x2 = int((x_center + width / 2) * img_width)\n",
    "        y2 = int((y_center + height / 2) * img_height)\n",
    "\n",
    "        labels_coordinates[class_mapping[int(class_id)]] = {\n",
    "            \"x1\": x1,\n",
    "            \"y1\": y1,\n",
    "            \"x2\": x2,\n",
    "            \"y2\": y2\n",
    "        }\n",
    "    return labels_coordinates\n",
    "\n",
    "convert_yolo_coordinates(annotation_path, 640, 640)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/combine_annotated_data/test/images/IMG_20240329_145238_jpg.rf.b940f962c22ad6c46a413a05dd23b9c2.jpg: 640x640 2 item_purchases, 1 shop_information, 514.7ms\n",
      "Speed: 1.1ms preprocess, 514.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "label\n",
      "shop_information accuracy: 0.6092026872860946\n",
      "item_purchase accuracy: 0.8045693700526141\n",
      "total not found\n",
      "date not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'shop_information': 0.6092026872860946,\n",
       " 'item_purchase': 0.8045693700526141,\n",
       " 'total': None,\n",
       " 'date': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_accuracy(model, image_path, annotation_path):\n",
    "    predictions = get_highest_prediction(model, image_path)\n",
    "    labels = convert_yolo_coordinates(annotation_path, 640, 640)\n",
    "    accuracies = {}\n",
    "\n",
    "    # Initialize all classes with None\n",
    "    for class_name in labels.keys():\n",
    "        accuracies[class_name] = None\n",
    "\n",
    "    #for each prediction, display the bounding box on the image and save it\n",
    "    print(f\"label\")\n",
    "    for key in labels:\n",
    "\n",
    "        if key not in predictions:\n",
    "            print(f\"{key} not found\")\n",
    "            accuracies[key] = None\n",
    "            continue\n",
    "\n",
    "        #load the image\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # print(f\"{key} : {predictions[key]['class_id']}\")\n",
    "\n",
    "        real_coordinates = labels[key]\n",
    "        predicted_coordinates = predictions[key][\"coordinates\"]\n",
    "        \n",
    "        # Draw the predicted bounding box with name\n",
    "        x1 = predicted_coordinates[\"x1\"]\n",
    "        y1 = predicted_coordinates[\"y1\"]\n",
    "        x2 = predicted_coordinates[\"x2\"]\n",
    "        y2 = predicted_coordinates[\"y2\"]\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image, \"prediction\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # Draw the real bounding box with name\n",
    "        x1 = real_coordinates[\"x1\"]\n",
    "        y1 = real_coordinates[\"y1\"]\n",
    "        x2 = real_coordinates[\"x2\"]\n",
    "        y2 = real_coordinates[\"y2\"]\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        cv2.putText(image, \"real\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "        # Calculate the common area between the two bounding boxes\n",
    "        x1 = max(real_coordinates[\"x1\"], predicted_coordinates[\"x1\"])\n",
    "        y1 = max(real_coordinates[\"y1\"], predicted_coordinates[\"y1\"])\n",
    "        x2 = min(real_coordinates[\"x2\"], predicted_coordinates[\"x2\"])\n",
    "        y2 = min(real_coordinates[\"y2\"], predicted_coordinates[\"y2\"])\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "        cv2.putText(image, \"intersection\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 255), 2)\n",
    "\n",
    "        #save picture\n",
    "        cv2.imwrite(f\"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/quick_test/prediction/{key}_prediction.png\", image)\n",
    "\n",
    "        #calculate the area of the union over the area of the prediction\n",
    "        if x2 > x1 and y2 > y1:  # Ensure there is an intersection\n",
    "            intersection_area = (x2 - x1) * (y2 - y1)\n",
    "            union_area = (real_coordinates[\"x2\"] - real_coordinates[\"x1\"]) * (real_coordinates[\"y2\"] - real_coordinates[\"y1\"]) + \\\n",
    "                         (predicted_coordinates[\"x2\"] - predicted_coordinates[\"x1\"]) * (predicted_coordinates[\"y2\"] - predicted_coordinates[\"y1\"]) - \\\n",
    "                         intersection_area\n",
    "            accuracy = intersection_area / union_area\n",
    "            accuracies[key] = accuracy\n",
    "            print(f\"{key} accuracy: {accuracy}\")\n",
    "        else:\n",
    "            accuracies[key] = None\n",
    "            print(f\"{key} has no intersection\")\n",
    "\n",
    "    return accuracies    \n",
    "    \n",
    "calculate_accuracy(model, image_path, annotation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/combine_annotated_data/valid/images/c5ea012e-IMG_20240111_204434_jpg.rf.893dda8a9aa1d4c2f0d62be6eb3343a9.jpg: 640x640 1 date, 3 item_purchases, 1 shop_information, 1 total, 515.5ms\n",
      "Speed: 0.8ms preprocess, 515.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "label\n",
      "shop_information accuracy: 0.8898568761667703\n",
      "item_purchase accuracy: 0.5757235643710096\n",
      "date accuracy: 0.7181467181467182\n",
      "total accuracy: 0.74051776038531\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/combine_annotated_data/valid/images/IMG_20240401_201118_jpg.rf.c948b0c3c0381619643571acc293a76d.jpg: 640x640 1 date, 2 item_purchases, 1 shop_information, 1 total, 506.0ms\n",
      "Speed: 0.8ms preprocess, 506.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "label\n",
      "shop_information accuracy: 0.8865773724707894\n",
      "item_purchase accuracy: 0.7680234783718259\n",
      "date accuracy: 0.9776536312849162\n",
      "total accuracy: 0.7924046416079062\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/combine_annotated_data/valid/images/IMG_20240310_165023_jpg.rf.1b1502136ae21b57ad2d226b033ee6cb.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 500.3ms\n",
      "Speed: 0.8ms preprocess, 500.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "label\n",
      "shop_information accuracy: 0.9046950890447922\n",
      "date accuracy: 0.6442892543365623\n",
      "item_purchase accuracy: 0.9161470732721158\n",
      "total accuracy: 0.7909844779927706\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/combine_annotated_data/valid/images/85f0bfdc-IMG_20231221_163244_jpg.rf.02fa8f61c2cfa7809ea33ce6708a3a0a.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 503.9ms\n",
      "Speed: 0.8ms preprocess, 503.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "label\n",
      "shop_information accuracy: 0.8623581647755303\n",
      "date accuracy: 0.8789473684210526\n",
      "item_purchase accuracy: 0.8466931216931217\n",
      "total accuracy: 0.7318681318681318\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/combine_annotated_data/valid/images/IMG_20240327_153429_jpg.rf.8ac75fe89cbeb2303d13172e825b8b87.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 503.4ms\n",
      "Speed: 0.8ms preprocess, 503.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "label\n",
      "shop_information accuracy: 0.904330430689574\n",
      "item_purchase accuracy: 0.8515744391826878\n",
      "total accuracy: 0.8465440710209258\n",
      "date accuracy: 0.8103638368246968\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/combine_annotated_data/valid/images/0676aa86-IMG_20231221_163400_jpg.rf.3ef3b7a564323dc7d96c82e257c4c400.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 523.0ms\n",
      "Speed: 0.8ms preprocess, 523.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "label\n",
      "shop_information accuracy: 0.900839552238806\n",
      "date accuracy: 0.8057095343680709\n",
      "item_purchase accuracy: 0.7378623247874471\n",
      "total accuracy: 0.7274096385542169\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/combine_annotated_data/valid/images/IMG_20240329_145238_jpg.rf.b940f962c22ad6c46a413a05dd23b9c2.jpg: 640x640 2 item_purchases, 1 shop_information, 512.3ms\n",
      "Speed: 0.9ms preprocess, 512.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "label\n",
      "shop_information accuracy: 0.6092026872860946\n",
      "item_purchase accuracy: 0.8045693700526141\n",
      "total not found\n",
      "date not found\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/combine_annotated_data/valid/images/IMG_20240401_200950_jpg.rf.64f2da99feb55b59b9cb7851a99b91f4.jpg: 640x640 1 date, 2 item_purchases, 1 shop_information, 1 total, 542.3ms\n",
      "Speed: 0.8ms preprocess, 542.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "label\n",
      "shop_information accuracy: 0.874265685935726\n",
      "date accuracy: 0.9791666666666666\n",
      "item_purchase accuracy: 0.7359795021315133\n",
      "total accuracy: 0.7914438502673797\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/combine_annotated_data/valid/images/IMG_20240310_165310_jpg.rf.8c689c8e5bbf96621f84c13f94b14fc0.jpg: 640x640 1 date, 3 item_purchases, 1 shop_information, 1 total, 505.5ms\n",
      "Speed: 0.7ms preprocess, 505.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "label\n",
      "shop_information accuracy: 0.8097469540768509\n",
      "item_purchase accuracy: 0.9412380814793412\n",
      "total accuracy: 0.875\n",
      "date accuracy: 0.6621880998080614\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/combine_annotated_data/valid/images/IMG_20240401_200823_jpg.rf.5d19e1f8ca079626fa74bbd3cbf52741.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 509.3ms\n",
      "Speed: 0.8ms preprocess, 509.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "label\n",
      "shop_information accuracy: 0.888479083762011\n",
      "date accuracy: 0.8841973244147158\n",
      "item_purchase accuracy: 0.6512120932804867\n",
      "total accuracy: 0.832625318606627\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/combine_annotated_data/valid/images/IMG_20240401_200742_jpg.rf.40f46456e1987116bd5ba9f06dfdfb59.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 504.9ms\n",
      "Speed: 0.8ms preprocess, 504.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "label\n",
      "shop_information accuracy: 0.8637691745799854\n",
      "date accuracy: 0.8551840269425066\n",
      "item_purchase accuracy: 0.7644215502946053\n",
      "total accuracy: 0.8774513727687505\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/combine_annotated_data/valid/images/567b168e-IMG_20240111_204431_jpg.rf.a64f68ff73fab83d9ab3c9250c193a91.jpg: 640x640 1 date, 6 item_purchases, 1 shop_information, 1 total, 505.8ms\n",
      "Speed: 0.8ms preprocess, 505.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "label\n",
      "shop_information accuracy: 0.8952583156404812\n",
      "date accuracy: 0.5661538461538461\n",
      "item_purchase accuracy: 0.5615486921826511\n",
      "total accuracy: 0.7561114269471291\n",
      "\n",
      "########################################################################\n",
      "Accuracy for version 0.2 :\n",
      "{'shop_information': {'average_accuracy': 0.8574482822222843, 'number_of_none': 0}, 'item_purchase': {'average_accuracy': 0.7629161075916183, 'number_of_none': 0}, 'date': {'average_accuracy': 0.7983636643061648, 'number_of_none': 1}, 'total': {'average_accuracy': 0.7965782445471952, 'number_of_none': 1}}\n",
      "Overall accuracy: 0.8038265746668156\n"
     ]
    }
   ],
   "source": [
    "image_folder_path = \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/combine_annotated_data/valid/images/\"\n",
    "annotation_folder_path = \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/combine_annotated_data/valid/labels/\"\n",
    "\n",
    "model_paths = {\n",
    "#    0.1 : \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/Server/model/versions/0.15/receipt_extractor.pt\",\n",
    "   0.2 : \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/Server/model/versions/0.2/receipt_extractor.pt\",\n",
    "}\n",
    "\n",
    "for version, model_path in model_paths.items():\n",
    "    model = YOLO(model_path)\n",
    "    #list all the images in the folder wihout the extension\n",
    "    images = [os.path.splitext(file)[0] for file in os.listdir(image_folder_path) if file.endswith(\".jpg\")]\n",
    "    all_accuracy = {}\n",
    "    for image in images:\n",
    "        image_path = os.path.join(image_folder_path, image + \".jpg\")\n",
    "        annotation_path = os.path.join(annotation_folder_path, image + \".txt\")\n",
    "        accuracies = calculate_accuracy(model, image_path, annotation_path)\n",
    "        all_accuracy[image] = accuracies\n",
    "\n",
    "    all_accuracy\n",
    "\n",
    "    # for each class calculate the average accuracy and the number of None\n",
    "    average_accuracy = {}\n",
    "    for key in all_accuracy[images[0]]:\n",
    "        average_accuracy[key] = {\n",
    "            \"average_accuracy\": 0,\n",
    "            \"number_of_none\": 0\n",
    "        }\n",
    "        for image in images:\n",
    "            if all_accuracy[image][key] is not None:\n",
    "                average_accuracy[key][\"average_accuracy\"] += all_accuracy[image][key]\n",
    "            else:\n",
    "                average_accuracy[key][\"number_of_none\"] += 1\n",
    "        average_accuracy[key][\"average_accuracy\"] = average_accuracy[key][\"average_accuracy\"] / (len(images) - average_accuracy[key][\"number_of_none\"])\n",
    "\n",
    "    print(\"\\n########################################################################\\nAccuracy for version\", version, \":\")\n",
    "    print(average_accuracy)\n",
    "\n",
    "    overall_accuracy = 0\n",
    "    for key in average_accuracy:\n",
    "        overall_accuracy += average_accuracy[key][\"average_accuracy\"]\n",
    "    overall_accuracy = overall_accuracy / len(average_accuracy)\n",
    "    print(f\"Overall accuracy: {overall_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "receipt_extractor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
