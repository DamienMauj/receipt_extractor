{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = YOLO(\"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/Server/receipt_server/model/versions/V0_2/receipt_extractor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/receipt extraction model.v2i.yolov8/test/images/0676aa86-IMG_20231221_163400_jpg.rf.3ef3b7a564323dc7d96c82e257c4c400.jpg\"\n",
    "annotation_path = \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/receipt extraction model.v2i.yolov8/test/labels/0676aa86-IMG_20231221_163400_jpg.rf.3ef3b7a564323dc7d96c82e257c4c400.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/receipt extraction model.v2i.yolov8/test/images/0676aa86-IMG_20231221_163400_jpg.rf.3ef3b7a564323dc7d96c82e257c4c400.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 650.7ms\n",
      "Speed: 2.2ms preprocess, 650.7ms inference, 315.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "{'shop_information': {'conf': tensor([0.8474]), 'class_label': 'shop_information', 'class_id': 2, 'coordinates': {'x1': 194, 'y1': 129, 'x2': 462, 'y2': 209}}, 'date': {'conf': tensor([0.7863]), 'class_label': 'date', 'class_id': 0, 'coordinates': {'x1': 150, 'y1': 212, 'x2': 314, 'y2': 234}}, 'total': {'conf': tensor([0.7854]), 'class_label': 'total', 'class_id': 3, 'coordinates': {'x1': 140, 'y1': 453, 'x2': 472, 'y2': 477}}, 'item_purchase': {'conf': tensor([0.6117]), 'class_label': 'item_purchase', 'class_id': 1, 'coordinates': {'x1': 132, 'y1': 245, 'x2': 491, 'y2': 411}}}\n"
     ]
    }
   ],
   "source": [
    "def get_highest_prediction(model, image_path)-> dict:\n",
    "    # Get the predictions\n",
    "    predictions = model(image_path)\n",
    "\n",
    "    # Get the highest prediction\n",
    "    highest_conf_detections = {}\n",
    "    for result in predictions:\n",
    "        # print(result)\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls)\n",
    "            class_label = result.names[int(box.cls)]\n",
    "            conf = box.conf\n",
    "\n",
    "            # Check if the class label is already in the dictionary\n",
    "            if class_label in highest_conf_detections:\n",
    "                # Check if the current confidence is higher than the one stored\n",
    "                if conf > highest_conf_detections[class_label][\"conf\"]:\n",
    "                    highest_conf_detections[class_label] = {\n",
    "                    \"conf\": conf,\n",
    "                    \"class_label\": class_label,\n",
    "                    \"class_id\": class_id,\n",
    "                    \"coordinates\": {\n",
    "                        \"x1\": int(box.xyxy[0][0]),\n",
    "                        \"y1\": int(box.xyxy[0][1]),\n",
    "                        \"x2\": int(box.xyxy[0][2]),\n",
    "                        \"y2\": int(box.xyxy[0][3]),\n",
    "                    }\n",
    "                }\n",
    "            else:\n",
    "                highest_conf_detections[class_label] = {\n",
    "                    \"conf\": conf,\n",
    "                    \"class_label\": class_label,\n",
    "                    \"class_id\": class_id,\n",
    "                    \"coordinates\": {\n",
    "                        \"x1\": int(box.xyxy[0][0]),\n",
    "                        \"y1\": int(box.xyxy[0][1]),\n",
    "                        \"x2\": int(box.xyxy[0][2]),\n",
    "                        \"y2\": int(box.xyxy[0][3]),\n",
    "                    }\n",
    "                }\n",
    "\n",
    "\n",
    "    return highest_conf_detections\n",
    "\n",
    "extracted_prediction = get_highest_prediction(model, image_path)\n",
    "print(extracted_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shop_information': {'x1': 200, 'y1': 132, 'x2': 461, 'y2': 206},\n",
       " 'date': {'x1': 158, 'y1': 213, 'x2': 311, 'y2': 232},\n",
       " 'item_purchase': {'x1': 150, 'y1': 241, 'x2': 483, 'y2': 380},\n",
       " 'total': {'x1': 149, 'y1': 454, 'x2': 471, 'y2': 472}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def convert_yolo_coordinates(file_path, img_width, img_height)->dict:\n",
    "\n",
    "    class_mapping = {\n",
    "        0: \"date\",\n",
    "        1: \"item_purchase\",\n",
    "        2: \"shop_information\",\n",
    "        3: \"total\",\n",
    "    }\n",
    "        \n",
    "    labels_coordinates = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) != 5:\n",
    "            continue  # Skip invalid lines\n",
    "\n",
    "        class_id, x_center, y_center, width, height = map(float, parts)\n",
    "\n",
    "        # Calculate the coordinates of the bounding box (x1, y1, x2, y2)\n",
    "        x1 = int((x_center - width / 2) * img_width)\n",
    "        y1 = int((y_center - height / 2) * img_height)\n",
    "        x2 = int((x_center + width / 2) * img_width)\n",
    "        y2 = int((y_center + height / 2) * img_height)\n",
    "\n",
    "        labels_coordinates[class_mapping[int(class_id)]] = {\n",
    "            \"x1\": x1,\n",
    "            \"y1\": y1,\n",
    "            \"x2\": x2,\n",
    "            \"y2\": y2\n",
    "        }\n",
    "    return labels_coordinates\n",
    "\n",
    "convert_yolo_coordinates(annotation_path, 640, 640)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/0676aa86-IMG_20231221_163400_jpg.rf.3ef3b7a564323dc7d96c82e257c4c400 copy.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 332.6ms\n",
      "Speed: 1.3ms preprocess, 332.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'shop_information': 0.9864864864864865,\n",
       " 'date': 0.974025974025974,\n",
       " 'item_purchase': 0.9605361751727584,\n",
       " 'total': 0.8466257668711656}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_accuracy(model, image_path, annotation_path):\n",
    "    predictions = get_highest_prediction(model, image_path)\n",
    "    labels = convert_yolo_coordinates(annotation_path, 640, 640)\n",
    "    accuracies = {}\n",
    "\n",
    "    # Initialize all classes with None\n",
    "    for class_name in labels.keys():\n",
    "        accuracies[class_name] = None\n",
    "\n",
    "    #for each prediction, display the bounding box on the image and save it\n",
    "    # print(f\"label\")\n",
    "    for key in labels:\n",
    "\n",
    "        if key not in predictions:\n",
    "            # print(f\"{key} not found\")\n",
    "            accuracies[key] = None\n",
    "            continue\n",
    "\n",
    "        #load the image\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # print(f\"{key} : {predictions[key]['class_id']}\")\n",
    "\n",
    "        real_coordinates = labels[key]\n",
    "        predicted_coordinates = predictions[key][\"coordinates\"]\n",
    "        \n",
    "        # Draw the predicted bounding box with name\n",
    "        x1 = predicted_coordinates[\"x1\"]\n",
    "        y1 = predicted_coordinates[\"y1\"]\n",
    "        x2 = predicted_coordinates[\"x2\"]\n",
    "        y2 = predicted_coordinates[\"y2\"]\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image, \"prediction\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # Draw the real bounding box with name\n",
    "        x1 = real_coordinates[\"x1\"]\n",
    "        y1 = real_coordinates[\"y1\"]\n",
    "        x2 = real_coordinates[\"x2\"]\n",
    "        y2 = real_coordinates[\"y2\"]\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        cv2.putText(image, \"real\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "        # Calculate the common area between the two bounding boxes\n",
    "        x1 = max(real_coordinates[\"x1\"], predicted_coordinates[\"x1\"])\n",
    "        y1 = max(real_coordinates[\"y1\"], predicted_coordinates[\"y1\"])\n",
    "        x2 = min(real_coordinates[\"x2\"], predicted_coordinates[\"x2\"])\n",
    "        y2 = min(real_coordinates[\"y2\"], predicted_coordinates[\"y2\"])\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "        cv2.putText(image, \"intersection\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 255), 2)\n",
    "\n",
    "        #save picture\n",
    "        cv2.imwrite(f\"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/quick_test/prediction/{key}_prediction.png\", image)\n",
    "\n",
    "        #calculate the area of the union over the area of the prediction\n",
    "        if x2 > x1 and y2 > y1:  # Ensure there is an intersection\n",
    "            intersection_area = (x2 - x1) * (y2 - y1)\n",
    "            union_area = (real_coordinates[\"x2\"] - real_coordinates[\"x1\"]) * (real_coordinates[\"y2\"] - real_coordinates[\"y1\"]) + \\\n",
    "                         (predicted_coordinates[\"x2\"] - predicted_coordinates[\"x1\"]) * (predicted_coordinates[\"y2\"] - predicted_coordinates[\"y1\"]) - \\\n",
    "                         intersection_area\n",
    "            accuracy = intersection_area / union_area\n",
    "            accuracies[key] = accuracy\n",
    "            # print(f\"{key} accuracy: {accuracy}\")\n",
    "        else:\n",
    "            accuracies[key] = None\n",
    "            # print(f\"{key} has no intersection\")\n",
    "\n",
    "    return accuracies    \n",
    "    \n",
    "calculate_accuracy(model, image_path, annotation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240329_145238_jpg.rf.b940f962c22ad6c46a413a05dd23b9c2 copy.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 507.4ms\n",
      "Speed: 0.7ms preprocess, 507.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/c5ea012e-IMG_20240111_204434_jpg.rf.893dda8a9aa1d4c2f0d62be6eb3343a9.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 503.0ms\n",
      "Speed: 0.9ms preprocess, 503.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240401_201118_jpg.rf.c948b0c3c0381619643571acc293a76d.jpg: 640x640 1 date, 2 item_purchases, 1 shop_information, 1 total, 499.4ms\n",
      "Speed: 1.0ms preprocess, 499.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/85f0bfdc-IMG_20231221_163244_jpg.rf.02fa8f61c2cfa7809ea33ce6708a3a0a copy.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 512.2ms\n",
      "Speed: 0.8ms preprocess, 512.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240310_165023_jpg.rf.1b1502136ae21b57ad2d226b033ee6cb.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 508.4ms\n",
      "Speed: 0.9ms preprocess, 508.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/85f0bfdc-IMG_20231221_163244_jpg.rf.02fa8f61c2cfa7809ea33ce6708a3a0a.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 518.8ms\n",
      "Speed: 1.0ms preprocess, 518.8ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240327_153429_jpg.rf.8ac75fe89cbeb2303d13172e825b8b87.jpg: 640x640 2 dates, 1 item_purchase, 1 shop_information, 1 total, 507.6ms\n",
      "Speed: 0.8ms preprocess, 507.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/0676aa86-IMG_20231221_163400_jpg.rf.3ef3b7a564323dc7d96c82e257c4c400.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 512.1ms\n",
      "Speed: 0.7ms preprocess, 512.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240329_145238_jpg.rf.b940f962c22ad6c46a413a05dd23b9c2.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 509.3ms\n",
      "Speed: 0.9ms preprocess, 509.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240401_200950_jpg.rf.64f2da99feb55b59b9cb7851a99b91f4.jpg: 640x640 1 date, 2 item_purchases, 1 shop_information, 1 total, 544.6ms\n",
      "Speed: 0.9ms preprocess, 544.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240310_165310_jpg.rf.8c689c8e5bbf96621f84c13f94b14fc0.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 503.4ms\n",
      "Speed: 0.9ms preprocess, 503.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240401_200823_jpg.rf.5d19e1f8ca079626fa74bbd3cbf52741.jpg: 640x640 1 date, 2 item_purchases, 1 shop_information, 1 total, 510.7ms\n",
      "Speed: 0.8ms preprocess, 510.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240401_200742_jpg.rf.40f46456e1987116bd5ba9f06dfdfb59.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 520.5ms\n",
      "Speed: 0.8ms preprocess, 520.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240401_200823_jpg.rf.5d19e1f8ca079626fa74bbd3cbf52741 copy.jpg: 640x640 1 date, 2 item_purchases, 1 shop_information, 1 total, 506.9ms\n",
      "Speed: 0.8ms preprocess, 506.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240327_153429_jpg.rf.8ac75fe89cbeb2303d13172e825b8b87 copy.jpg: 640x640 2 dates, 1 item_purchase, 1 shop_information, 1 total, 505.4ms\n",
      "Speed: 0.8ms preprocess, 505.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/567b168e-IMG_20240111_204431_jpg.rf.a64f68ff73fab83d9ab3c9250c193a91.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 506.2ms\n",
      "Speed: 0.8ms preprocess, 506.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/0676aa86-IMG_20231221_163400_jpg.rf.3ef3b7a564323dc7d96c82e257c4c400 copy.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 577.3ms\n",
      "Speed: 0.8ms preprocess, 577.3ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "########################################################################\n",
      "Accuracy for version 0.3 :\n",
      "{'shop_information': {'average_accuracy': 0.894708871457897, 'number_of_none': 0}, 'item_purchase': {'average_accuracy': 0.8867944546096387, 'number_of_none': 0}, 'total': {'average_accuracy': 0.8736616051249958, 'number_of_none': 0}, 'date': {'average_accuracy': 0.8209201093717151, 'number_of_none': 0}}\n",
      "Overall accuracy: 0.8690212601410616\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240329_145238_jpg.rf.b940f962c22ad6c46a413a05dd23b9c2 copy.jpg: 640x640 2 dates, 1 item_purchase, 1 shop_information, 1 total, 317.7ms\n",
      "Speed: 0.9ms preprocess, 317.7ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/c5ea012e-IMG_20240111_204434_jpg.rf.893dda8a9aa1d4c2f0d62be6eb3343a9.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 304.0ms\n",
      "Speed: 0.8ms preprocess, 304.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240401_201118_jpg.rf.c948b0c3c0381619643571acc293a76d.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 306.8ms\n",
      "Speed: 0.8ms preprocess, 306.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/85f0bfdc-IMG_20231221_163244_jpg.rf.02fa8f61c2cfa7809ea33ce6708a3a0a copy.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 303.7ms\n",
      "Speed: 0.9ms preprocess, 303.7ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240310_165023_jpg.rf.1b1502136ae21b57ad2d226b033ee6cb.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 309.1ms\n",
      "Speed: 0.8ms preprocess, 309.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/85f0bfdc-IMG_20231221_163244_jpg.rf.02fa8f61c2cfa7809ea33ce6708a3a0a.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 331.6ms\n",
      "Speed: 0.7ms preprocess, 331.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240327_153429_jpg.rf.8ac75fe89cbeb2303d13172e825b8b87.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 307.5ms\n",
      "Speed: 0.8ms preprocess, 307.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/0676aa86-IMG_20231221_163400_jpg.rf.3ef3b7a564323dc7d96c82e257c4c400.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 307.4ms\n",
      "Speed: 0.7ms preprocess, 307.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240329_145238_jpg.rf.b940f962c22ad6c46a413a05dd23b9c2.jpg: 640x640 2 dates, 1 item_purchase, 1 shop_information, 1 total, 305.5ms\n",
      "Speed: 0.8ms preprocess, 305.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240401_200950_jpg.rf.64f2da99feb55b59b9cb7851a99b91f4.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 310.8ms\n",
      "Speed: 0.7ms preprocess, 310.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240310_165310_jpg.rf.8c689c8e5bbf96621f84c13f94b14fc0.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 308.4ms\n",
      "Speed: 0.7ms preprocess, 308.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240401_200823_jpg.rf.5d19e1f8ca079626fa74bbd3cbf52741.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 312.8ms\n",
      "Speed: 0.8ms preprocess, 312.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240401_200742_jpg.rf.40f46456e1987116bd5ba9f06dfdfb59.jpg: 640x640 1 date, 1 item_purchase, 2 shop_informations, 1 total, 303.8ms\n",
      "Speed: 0.8ms preprocess, 303.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240401_200823_jpg.rf.5d19e1f8ca079626fa74bbd3cbf52741 copy.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 310.0ms\n",
      "Speed: 0.8ms preprocess, 310.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/IMG_20240327_153429_jpg.rf.8ac75fe89cbeb2303d13172e825b8b87 copy.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 360.0ms\n",
      "Speed: 0.7ms preprocess, 360.0ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/567b168e-IMG_20240111_204431_jpg.rf.a64f68ff73fab83d9ab3c9250c193a91.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 309.1ms\n",
      "Speed: 0.8ms preprocess, 309.1ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/0676aa86-IMG_20231221_163400_jpg.rf.3ef3b7a564323dc7d96c82e257c4c400 copy.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 311.2ms\n",
      "Speed: 0.7ms preprocess, 311.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "########################################################################\n",
      "Accuracy for version 0.35 :\n",
      "{'shop_information': {'average_accuracy': 0.9086081749833191, 'number_of_none': 0}, 'item_purchase': {'average_accuracy': 0.9006391280141678, 'number_of_none': 0}, 'total': {'average_accuracy': 0.8437334905062626, 'number_of_none': 0}, 'date': {'average_accuracy': 0.8267780425051116, 'number_of_none': 0}}\n",
      "Overall accuracy: 0.8699397090022152\n"
     ]
    }
   ],
   "source": [
    "image_folder_path = \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/images/\"\n",
    "annotation_folder_path = \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/testting_data/labels/\"\n",
    "\n",
    "model_paths = {\n",
    "#    0.1 : \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/Server/model/versions/0.15/receipt_extractor.pt\",\n",
    "#    0.2 : \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/Server/receipt_server/model/versions/V0_2/receipt_extractor.pt\",\n",
    "   0.3: \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/Server/receipt_server/model/versions/V0_3/receipt_extractor.pt\",\n",
    "   0.35: \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/Server/receipt_server/model/versions/V9_test/receipt_extractor.pt\",\n",
    "}\n",
    "\n",
    "for version, model_path in model_paths.items():\n",
    "    model = YOLO(model_path)\n",
    "    #list all the images in the folder wihout the extension\n",
    "    images = [os.path.splitext(file)[0] for file in os.listdir(image_folder_path) if file.endswith(\".jpg\")]\n",
    "    all_accuracy = {}\n",
    "    for image in images:\n",
    "        image_path = os.path.join(image_folder_path, image + \".jpg\")\n",
    "        annotation_path = os.path.join(annotation_folder_path, image + \".txt\")\n",
    "        accuracies = calculate_accuracy(model, image_path, annotation_path)\n",
    "        all_accuracy[image] = accuracies\n",
    "\n",
    "    all_accuracy\n",
    "\n",
    "    # for each class calculate the average accuracy and the number of None\n",
    "    average_accuracy = {}\n",
    "    for key in all_accuracy[images[0]]:\n",
    "        average_accuracy[key] = {\n",
    "            \"average_accuracy\": 0,\n",
    "            \"number_of_none\": 0\n",
    "        }\n",
    "        for image in images:\n",
    "            if all_accuracy[image][key] is not None:\n",
    "                average_accuracy[key][\"average_accuracy\"] += all_accuracy[image][key]\n",
    "            else:\n",
    "                average_accuracy[key][\"number_of_none\"] += 1\n",
    "        average_accuracy[key][\"average_accuracy\"] = average_accuracy[key][\"average_accuracy\"] / (len(images) - average_accuracy[key][\"number_of_none\"])\n",
    "\n",
    "    print(\"\\n########################################################################\\nAccuracy for version\", version, \":\")\n",
    "    print(average_accuracy)\n",
    "\n",
    "    overall_accuracy = 0\n",
    "    for key in average_accuracy:\n",
    "        overall_accuracy += average_accuracy[key][\"average_accuracy\"]\n",
    "    overall_accuracy = overall_accuracy / len(average_accuracy)\n",
    "    print(f\"Overall accuracy: {overall_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "receipt_extractor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
