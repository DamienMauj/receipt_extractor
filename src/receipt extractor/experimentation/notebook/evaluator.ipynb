{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = YOLO(\"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/Server/receipt_server/model/versions/V0_2/receipt_extractor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/receipt extraction model.v2i.yolov8/test/images/0676aa86-IMG_20231221_163400_jpg.rf.3ef3b7a564323dc7d96c82e257c4c400.jpg\"\n",
    "annotation_path = \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/receipt extraction model.v2i.yolov8/test/labels/0676aa86-IMG_20231221_163400_jpg.rf.3ef3b7a564323dc7d96c82e257c4c400.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/receipt extraction model.v2i.yolov8/test/images/0676aa86-IMG_20231221_163400_jpg.rf.3ef3b7a564323dc7d96c82e257c4c400.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 556.4ms\n",
      "Speed: 0.9ms preprocess, 556.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "{'shop_information': {'conf': tensor([0.8474]), 'class_label': 'shop_information', 'class_id': 2, 'coordinates': {'x1': 194, 'y1': 129, 'x2': 462, 'y2': 209}}, 'date': {'conf': tensor([0.7863]), 'class_label': 'date', 'class_id': 0, 'coordinates': {'x1': 150, 'y1': 212, 'x2': 314, 'y2': 234}}, 'total': {'conf': tensor([0.7854]), 'class_label': 'total', 'class_id': 3, 'coordinates': {'x1': 140, 'y1': 453, 'x2': 472, 'y2': 477}}, 'item_purchase': {'conf': tensor([0.6117]), 'class_label': 'item_purchase', 'class_id': 1, 'coordinates': {'x1': 132, 'y1': 245, 'x2': 491, 'y2': 411}}}\n"
     ]
    }
   ],
   "source": [
    "def get_highest_prediction(model, image_path)-> dict:\n",
    "    # Get the predictions\n",
    "    predictions = model(image_path)\n",
    "\n",
    "    # Get the highest prediction\n",
    "    highest_conf_detections = {}\n",
    "    for result in predictions:\n",
    "        # print(result)\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls)\n",
    "            class_label = result.names[int(box.cls)]\n",
    "            conf = box.conf\n",
    "\n",
    "            # Check if the class label is already in the dictionary\n",
    "            if class_label in highest_conf_detections:\n",
    "                # Check if the current confidence is higher than the one stored\n",
    "                if conf > highest_conf_detections[class_label][\"conf\"]:\n",
    "                    highest_conf_detections[class_label] = {\n",
    "                    \"conf\": conf,\n",
    "                    \"class_label\": class_label,\n",
    "                    \"class_id\": class_id,\n",
    "                    \"coordinates\": {\n",
    "                        \"x1\": int(box.xyxy[0][0]),\n",
    "                        \"y1\": int(box.xyxy[0][1]),\n",
    "                        \"x2\": int(box.xyxy[0][2]),\n",
    "                        \"y2\": int(box.xyxy[0][3]),\n",
    "                    }\n",
    "                }\n",
    "            else:\n",
    "                highest_conf_detections[class_label] = {\n",
    "                    \"conf\": conf,\n",
    "                    \"class_label\": class_label,\n",
    "                    \"class_id\": class_id,\n",
    "                    \"coordinates\": {\n",
    "                        \"x1\": int(box.xyxy[0][0]),\n",
    "                        \"y1\": int(box.xyxy[0][1]),\n",
    "                        \"x2\": int(box.xyxy[0][2]),\n",
    "                        \"y2\": int(box.xyxy[0][3]),\n",
    "                    }\n",
    "                }\n",
    "\n",
    "\n",
    "    return highest_conf_detections\n",
    "\n",
    "extracted_prediction = get_highest_prediction(model, image_path)\n",
    "print(extracted_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shop_information': {'x1': 200, 'y1': 132, 'x2': 461, 'y2': 206},\n",
       " 'date': {'x1': 158, 'y1': 213, 'x2': 311, 'y2': 232},\n",
       " 'item_purchase': {'x1': 150, 'y1': 241, 'x2': 483, 'y2': 380},\n",
       " 'total': {'x1': 149, 'y1': 454, 'x2': 471, 'y2': 472}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def convert_yolo_coordinates(file_path, img_width, img_height)->dict:\n",
    "\n",
    "    class_mapping = {\n",
    "        0: \"date\",\n",
    "        1: \"item_purchase\",\n",
    "        2: \"shop_information\",\n",
    "        3: \"total\",\n",
    "    }\n",
    "        \n",
    "    labels_coordinates = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) != 5:\n",
    "            continue  # Skip invalid lines\n",
    "\n",
    "        class_id, x_center, y_center, width, height = map(float, parts)\n",
    "\n",
    "        # Calculate the coordinates of the bounding box (x1, y1, x2, y2)\n",
    "        x1 = int((x_center - width / 2) * img_width)\n",
    "        y1 = int((y_center - height / 2) * img_height)\n",
    "        x2 = int((x_center + width / 2) * img_width)\n",
    "        y2 = int((y_center + height / 2) * img_height)\n",
    "\n",
    "        labels_coordinates[class_mapping[int(class_id)]] = {\n",
    "            \"x1\": x1,\n",
    "            \"y1\": y1,\n",
    "            \"x2\": x2,\n",
    "            \"y2\": y2\n",
    "        }\n",
    "    return labels_coordinates\n",
    "\n",
    "convert_yolo_coordinates(annotation_path, 640, 640)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/receipt extraction model.v2i.yolov8/test/images/0676aa86-IMG_20231221_163400_jpg.rf.3ef3b7a564323dc7d96c82e257c4c400.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 500.9ms\n",
      "Speed: 0.8ms preprocess, 500.9ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'shop_information': 1.0,\n",
       " 'date': 1.0,\n",
       " 'item_purchase': 0.9712230215827338,\n",
       " 'total': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_accuracy(model, image_path, annotation_path, image_name):\n",
    "    predictions = get_highest_prediction(model, image_path)\n",
    "    labels = convert_yolo_coordinates(annotation_path, 640, 640)\n",
    "    accuracies = {}\n",
    "    is_detected = {}\n",
    "\n",
    "    # Initialize all classes with None\n",
    "    for class_name in labels.keys():\n",
    "        accuracies[class_name] = None\n",
    "\n",
    "    #for each prediction, display the bounding box on the image and save it\n",
    "    # print(f\"label\")\n",
    "    for key in labels:\n",
    "\n",
    "        is_detected[key] = True\n",
    "\n",
    "        if key not in predictions:\n",
    "            print(f\"{key} not found\")\n",
    "            accuracies[key] = None\n",
    "            continue\n",
    "\n",
    "        #load the image\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # print(f\"{key} : {predictions[key]['class_id']}\")\n",
    "\n",
    "        real_coordinates = labels[key]\n",
    "        predicted_coordinates = predictions[key][\"coordinates\"]\n",
    "        \n",
    "        # Draw the predicted bounding box with name\n",
    "        x1 = predicted_coordinates[\"x1\"]\n",
    "        y1 = predicted_coordinates[\"y1\"]\n",
    "        x2 = predicted_coordinates[\"x2\"]\n",
    "        y2 = predicted_coordinates[\"y2\"]\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image, \"prediction\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # Draw the real bounding box with name\n",
    "        x1 = real_coordinates[\"x1\"]\n",
    "        y1 = real_coordinates[\"y1\"]\n",
    "        x2 = real_coordinates[\"x2\"]\n",
    "        y2 = real_coordinates[\"y2\"]\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        cv2.putText(image, \"real\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "        # Calculate the common area between the two bounding boxes if they intersect\n",
    "        x1 = max(real_coordinates[\"x1\"], predicted_coordinates[\"x1\"])\n",
    "        y1 = max(real_coordinates[\"y1\"], predicted_coordinates[\"y1\"])\n",
    "        x2 = min(real_coordinates[\"x2\"], predicted_coordinates[\"x2\"])\n",
    "        y2 = min(real_coordinates[\"y2\"], predicted_coordinates[\"y2\"])\n",
    "        if x1 >= x2 or y1 >= y2:\n",
    "            # print(f\"{key} not intersecting\")\n",
    "            accuracies[key] = 0\n",
    "            is_detected[key] = False\n",
    "            # cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "            # cv2.putText(image, \" not intersection\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 255), 2)\n",
    "            accuracies[key] = 0\n",
    "            continue\n",
    "        # else:\n",
    "        #     cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "        #     cv2.putText(image, \"intersection\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 255), 2)\n",
    "\n",
    "        #save picture\n",
    "        # print(f\"saving prediction for {image_name} {key}\")\n",
    "        # print(f\"at /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/prediction/{image_name}/{key}_prediction.png\")\n",
    "        if not os.path.exists(f\"./evaluation_data/temp/{image_name.replace('.','_')}\"):\n",
    "            os.makedirs(f\"./evaluation_data/temp/{image_name.replace('.','_')}\")\n",
    "        cv2.imwrite(f\"./evaluation_data/temp/{image_name.replace('.','_')}/{key}_prediction.png\", image)\n",
    "\n",
    "        #calculate the area of the union over the area of the label\n",
    "        area_intersection = (x2 - x1) * (y2 - y1)\n",
    "        area_label = (real_coordinates[\"x2\"] - real_coordinates[\"x1\"]) * (real_coordinates[\"y2\"] - real_coordinates[\"y1\"])\n",
    "        accuracy = area_intersection / area_label\n",
    "        accuracies[key] = accuracy\n",
    "\n",
    "    return accuracies\n",
    "calculate_accuracy(model, image_path, annotation_path, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/85f0bfdc-IMG_20231221_163244_jpg.rf.d3d144d5ab06c81c8e82d70bd5370964.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 531.1ms\n",
      "Speed: 0.9ms preprocess, 531.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/0676aa86-IMG_20231221_163400_jpg.rf.d64d852e1a1cc4cac2fc25a440a4688e.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 502.6ms\n",
      "Speed: 0.8ms preprocess, 502.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/IMG_20240417_133105_jpg.rf.3dddf23822f73f64151ace8ee9ba1f75.jpg: 640x640 1 item_purchase, 512.7ms\n",
      "Speed: 0.8ms preprocess, 512.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "shop_information not found\n",
      "date not found\n",
      "total not found\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/IMG_20240327_153429_jpg.rf.1a92fdd793ca5b996f07397ac04cad38.jpg: 640x640 2 dates, 1 item_purchase, 1 shop_information, 1 total, 514.3ms\n",
      "Speed: 0.8ms preprocess, 514.3ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/IMG_20240327_110815_jpg.rf.cb33f0892a7358f46c3e002f1264f237.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 513.4ms\n",
      "Speed: 0.9ms preprocess, 513.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/IMG_20240401_200742_jpg.rf.b8b12b237ef3087b2d14e28900606044.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 508.1ms\n",
      "Speed: 0.8ms preprocess, 508.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/IMG_20240418_091608_jpg.rf.2ea3d16a80cf502ec3ab37ebabe61f56.jpg: 640x640 1 item_purchase, 1 shop_information, 1 total, 506.2ms\n",
      "Speed: 0.9ms preprocess, 506.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "date not found\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/IMG_20240415_131806_jpg.rf.e4b871f51bc2dd35892797931547a4ed.jpg: 640x640 2 dates, 1 item_purchase, 1 shop_information, 2 totals, 502.1ms\n",
      "Speed: 0.8ms preprocess, 502.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/567b168e-IMG_20240111_204431_jpg.rf.6cf0b497e8a4d654f36fa3112e60e6a0.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 502.2ms\n",
      "Speed: 1.0ms preprocess, 502.2ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/IMG_20240418_184125_jpg.rf.22d026cb1b7543081c8eece4ebe5cde7.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 2 totals, 513.1ms\n",
      "Speed: 0.8ms preprocess, 513.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/IMG_20240310_165310_jpg.rf.515c7373001708e130ebc6e662ca84de.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 520.5ms\n",
      "Speed: 0.9ms preprocess, 520.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/IMG_20240418_184154_jpg.rf.44254311af62427f78491bc2b44decb2.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 522.5ms\n",
      "Speed: 0.8ms preprocess, 522.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "total not found\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/IMG_20240401_201118_jpg.rf.841c4f25665808b739b75faee5fe33aa.jpg: 640x640 1 date, 2 item_purchases, 1 shop_information, 1 total, 509.0ms\n",
      "Speed: 0.8ms preprocess, 509.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/IMG_20240418_110707_jpg.rf.9f2cf99890d28a04392225428b61afd8.jpg: 640x640 2 dates, 1 item_purchase, 1 shop_information, 1 total, 507.8ms\n",
      "Speed: 1.0ms preprocess, 507.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/IMG_20240401_200823_jpg.rf.2197c0e3f33395c73ea63998d684cbee.jpg: 640x640 1 date, 2 item_purchases, 1 shop_information, 1 total, 530.0ms\n",
      "Speed: 0.8ms preprocess, 530.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/IMG_20240401_200950_jpg.rf.d9e4643cc9baf82a84c9867d30f52bc6.jpg: 640x640 1 date, 2 item_purchases, 1 shop_information, 1 total, 506.2ms\n",
      "Speed: 0.9ms preprocess, 506.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/c5ea012e-IMG_20240111_204434_jpg.rf.cb78335d8bc3c92821dcf977be2602fc.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 507.4ms\n",
      "Speed: 1.0ms preprocess, 507.4ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/IMG_20240329_145238_jpg.rf.939db06fec5e4a63101971da25f9677f.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 502.6ms\n",
      "Speed: 0.9ms preprocess, 502.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "image 1/1 /Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/IMG_20240310_165023_jpg.rf.aede72376da2504ecc56c108964adab9.jpg: 640x640 1 date, 1 item_purchase, 1 shop_information, 1 total, 507.8ms\n",
      "Speed: 0.8ms preprocess, 507.8ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "########################################################################\n",
      "Accuracy for version 0.3 :\n",
      "{'date': {'average_accuracy': 0.8632077244214241, 'number_of_none': 2}, 'item_purchase': {'average_accuracy': 0.8136411639807403, 'number_of_none': 0}, 'shop_information': {'average_accuracy': 0.9615977077502735, 'number_of_none': 1}, 'total': {'average_accuracy': 0.8690074387532404, 'number_of_none': 2}}\n",
      "Overall accuracy: 0.8768635087264195\n"
     ]
    }
   ],
   "source": [
    "image_folder_path = \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/images/\"\n",
    "annotation_folder_path = \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/receipt extractor/experimentation/notebook/evaluation_data/labels/\"\n",
    "\n",
    "model_paths = {\n",
    "#    0.1 : \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/Server/receipt_server/model/versions/V0_1/receipt_extractor.pt\",\n",
    "#    0.2 : \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/Server/receipt_server/model/versions/V0_2/receipt_extractor.pt\",\n",
    "   0.3: \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/Server/receipt_server/model/versions/V0_3/receipt_extractor.pt\",\n",
    "#    \"v9\": \"/Users/damienmaujean/Library/CloudStorage/OneDrive-Personal/university/MiddleSex University/Year 3/CST3990 Individual Project/src/Server/receipt_server/model/versions/V9_test/receipt_extractor.pt\",\n",
    "}\n",
    "\n",
    "for version, model_path in model_paths.items():\n",
    "    model = YOLO(model_path)\n",
    "    #list all the images in the folder wihout the extension\n",
    "    images = [os.path.splitext(file)[0] for file in os.listdir(image_folder_path) if file.endswith(\".jpg\")]\n",
    "    all_accuracy = {}\n",
    "    for image in images:\n",
    "        image_path = os.path.join(image_folder_path, image + \".jpg\")\n",
    "        annotation_path = os.path.join(annotation_folder_path, image + \".txt\")\n",
    "        accuracies = calculate_accuracy(model, image_path, annotation_path, image)\n",
    "        all_accuracy[image] = accuracies                     \n",
    "\n",
    "    # for each class calculate the average accuracy and the number of None\n",
    "    average_accuracy = {}\n",
    "    for key in [\"date\", \"item_purchase\", \"shop_information\", \"total\"]:\n",
    "        average_accuracy[key] = {\n",
    "            \"average_accuracy\": 0,\n",
    "            \"number_of_none\": 0\n",
    "        }\n",
    "        for image in images:\n",
    "            if all_accuracy[image][key] is not None:\n",
    "                average_accuracy[key][\"average_accuracy\"] += all_accuracy[image][key]\n",
    "            else:\n",
    "                average_accuracy[key][\"number_of_none\"] += 1\n",
    "        average_accuracy[key][\"average_accuracy\"] = average_accuracy[key][\"average_accuracy\"] / (len(images) - average_accuracy[key][\"number_of_none\"])\n",
    "\n",
    "    print(\"\\n########################################################################\\nAccuracy for version\", version, \":\")\n",
    "    print(average_accuracy)\n",
    "\n",
    "    overall_accuracy = 0\n",
    "    for key in average_accuracy:\n",
    "        overall_accuracy += average_accuracy[key][\"average_accuracy\"]\n",
    "    overall_accuracy = overall_accuracy / len(average_accuracy)\n",
    "\n",
    "    print(f\"Overall accuracy: {overall_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "receipt_extractor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
